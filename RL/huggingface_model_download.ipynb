{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading Hugging Face Models Locally\n",
    "\n",
    "This notebook will guide you through the process of downloading models from Hugging Face to your local device using the `huggingface-cli` command-line tool.\n",
    "\n",
    "## Table of Contents\n",
    "1. Installing Required Libraries\n",
    "2. Creating a Hugging Face Account and Getting an Access Token\n",
    "3. Logging in with huggingface-cli\n",
    "4. Downloading Models\n",
    "5. Using Downloaded Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installing Required Libraries\n",
    "\n",
    "First, we need to install the Hugging Face Hub library which includes the CLI tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the huggingface_hub library\n",
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on what you want to do with the models, you may also need:\n",
    "- `transformers` - for working with transformer models\n",
    "- `torch` or `tensorflow` - for running the models\n",
    "- `diffusers` - for working with diffusion models (like Stable Diffusion)\n",
    "- `datasets` - for working with Hugging Face datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional common libraries (optional, based on your needs)\n",
    "!pip install transformers torch datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating a Hugging Face Account and Getting an Access Token\n",
    "\n",
    "### Steps to get your access token:\n",
    "\n",
    "1. **Create an account** (if you don't have one):\n",
    "   - Go to [huggingface.co](https://huggingface.co)\n",
    "   - Click \"Sign Up\" and create your account\n",
    "\n",
    "2. **Generate an access token**:\n",
    "   - Log in to your Hugging Face account\n",
    "   - Click on your profile picture (top right)\n",
    "   - Go to **Settings** â†’ **Access Tokens**\n",
    "   - Click **\"New token\"**\n",
    "   - Give it a name (e.g., \"local-downloads\")\n",
    "   - Select the token type:\n",
    "     - **Read**: For downloading public and private models you have access to\n",
    "     - **Write**: If you also want to upload models\n",
    "   - Click **\"Generate token\"**\n",
    "   - **Important**: Copy the token immediately - you won't be able to see it again!\n",
    "\n",
    "### Token Types:\n",
    "- **Read tokens**: Can download models and datasets\n",
    "- **Write tokens**: Can upload and modify models and datasets\n",
    "- **Fine-grained tokens**: Custom permissions for specific repositories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Logging in with huggingface-cli\n",
    "\n",
    "There are two main ways to authenticate with Hugging Face:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Using the Command Line (Recommended)\n",
    "\n",
    "Open your terminal and run:\n",
    "\n",
    "```bash\n",
    "huggingface-cli login\n",
    "```\n",
    "\n",
    "You'll be prompted to:\n",
    "1. Enter your access token\n",
    "2. Choose whether to add the token as a git credential (recommended: yes)\n",
    "\n",
    "The token will be saved securely on your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also run it from within the notebook:\n",
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Programmatic Login (Alternative)\n",
    "\n",
    "You can also log in directly from Python code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Option 1: Interactive - will prompt for token\n",
    "login()\n",
    "\n",
    "# Option 2: Pass token directly (not recommended for notebooks you'll share)\n",
    "# login(token=\"your_token_here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Your Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import whoami\n",
    "\n",
    "# Check if you're logged in\n",
    "try:\n",
    "    user_info = whoami()\n",
    "    print(f\"Successfully logged in as: {user_info['name']}\")\n",
    "    print(f\"Email: {user_info.get('email', 'N/A')}\")\n",
    "except Exception as e:\n",
    "    print(f\"Not logged in or error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Downloading Models\n",
    "\n",
    "### Finding Models on Hugging Face\n",
    "\n",
    "1. Go to [huggingface.co/models](https://huggingface.co/models)\n",
    "2. Browse or search for models\n",
    "3. Filter by:\n",
    "   - Task (text generation, image classification, etc.)\n",
    "   - Library (transformers, diffusers, etc.)\n",
    "   - Language\n",
    "   - License\n",
    "4. Click on a model to see its page\n",
    "5. Copy the model ID (e.g., `bert-base-uncased`, `meta-llama/Llama-2-7b-hf`)\n",
    "\n",
    "### Method 1: Download Using CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download an entire model repository\n",
    "!huggingface-cli download bert-base-uncased\n",
    "\n",
    "# Download a specific model with all files\n",
    "!huggingface-cli download gpt2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Download Using Python API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download, snapshot_download\n",
    "\n",
    "# Download a single file from a model\n",
    "file_path = hf_hub_download(\n",
    "    repo_id=\"bert-base-uncased\",\n",
    "    filename=\"config.json\"\n",
    ")\n",
    "print(f\"Downloaded file to: {file_path}\")\n",
    "\n",
    "# Download entire model repository\n",
    "model_path = snapshot_download(\n",
    "    repo_id=\"gpt2\"\n",
    ")\n",
    "print(f\"Downloaded model to: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Specific Files or Revisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Download only specific file patterns\n",
    "model_path = snapshot_download(\n",
    "    repo_id=\"gpt2\",\n",
    "    allow_patterns=[\"*.json\", \"*.txt\"],  # Only download JSON and TXT files\n",
    ")\n",
    "\n",
    "# Download a specific revision (branch or tag)\n",
    "model_path = snapshot_download(\n",
    "    repo_id=\"gpt2\",\n",
    "    revision=\"main\"  # or a specific commit hash\n",
    ")\n",
    "\n",
    "# Download to a specific local directory\n",
    "model_path = snapshot_download(\n",
    "    repo_id=\"gpt2\",\n",
    "    local_dir=\"./my_models/gpt2\",\n",
    "    local_dir_use_symlinks=False  # Copy files instead of using symlinks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where Are Models Downloaded?\n",
    "\n",
    "By default, models are cached in:\n",
    "- **Linux/Mac**: `~/.cache/huggingface/hub/`\n",
    "- **Windows**: `C:\\Users\\<username>\\.cache\\huggingface\\hub\\`\n",
    "\n",
    "You can change this by setting the `HF_HOME` environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Check your cache directory\n",
    "cache_dir = os.getenv('HF_HOME', Path.home() / '.cache' / 'huggingface')\n",
    "print(f\"Hugging Face cache directory: {cache_dir}\")\n",
    "\n",
    "# To change it (set before importing huggingface_hub):\n",
    "# os.environ['HF_HOME'] = '/path/to/your/cache'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Using Downloaded Models\n",
    "\n",
    "Once models are downloaded, you can use them with the transformers library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load a model - it will use the cached version if already downloaded\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Successfully loaded {model_name}\")\n",
    "print(f\"Model has {model.num_parameters():,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Text Generation with GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create a text generation pipeline\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "\n",
    "# Generate text\n",
    "result = generator(\n",
    "    \"Artificial intelligence is\",\n",
    "    max_length=50,\n",
    "    num_return_sequences=1\n",
    ")\n",
    "\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Local Model Path\n",
    "\n",
    "If you've downloaded a model to a specific directory, you can load it directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Load from a local directory\n",
    "local_model_path = \"./my_models/gpt2\"\n",
    "\n",
    "# Make sure the path exists first\n",
    "if os.path.exists(local_model_path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "    model = AutoModel.from_pretrained(local_model_path)\n",
    "    print(\"Model loaded from local path\")\n",
    "else:\n",
    "    print(f\"Path {local_model_path} does not exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Tips\n",
    "\n",
    "### Downloading Gated Models\n",
    "\n",
    "Some models (like Llama 2) require you to:\n",
    "1. Accept the license on the model's Hugging Face page\n",
    "2. Use a token with read permissions\n",
    "\n",
    "### Managing Cache\n",
    "\n",
    "To see what's in your cache and manage it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import scan_cache_dir\n",
    "\n",
    "# Scan your cache\n",
    "cache_info = scan_cache_dir()\n",
    "\n",
    "print(f\"Total cache size: {cache_info.size_on_disk / (1024**3):.2f} GB\")\n",
    "print(f\"Number of repos: {len(cache_info.repos)}\")\n",
    "\n",
    "# List all cached repos\n",
    "for repo in cache_info.repos:\n",
    "    print(f\"- {repo.repo_id}: {repo.size_on_disk / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Unused Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import scan_cache_dir\n",
    "\n",
    "# Get cache info\n",
    "cache_info = scan_cache_dir()\n",
    "\n",
    "# Delete a specific repo from cache\n",
    "# Uncomment and modify to use:\n",
    "# delete_strategy = cache_info.delete_revisions(\"model-name\")\n",
    "# delete_strategy.execute()\n",
    "\n",
    "print(\"Cache management commands ready to use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you've learned:\n",
    "1. How to install the necessary Hugging Face libraries\n",
    "2. How to create an account and get an access token\n",
    "3. How to log in using `huggingface-cli login`\n",
    "4. Multiple methods to download models from Hugging Face\n",
    "5. How to use downloaded models in your projects\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Hugging Face Hub Documentation](https://huggingface.co/docs/huggingface_hub/index)\n",
    "- [Transformers Documentation](https://huggingface.co/docs/transformers/index)\n",
    "- [Hugging Face Models](https://huggingface.co/models)\n",
    "- [Hugging Face CLI Guide](https://huggingface.co/docs/huggingface_hub/guides/cli)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
